import streamlit as st
import os
from utils import document_parser, text_cleaner, summarizer, embedder, search_faiss, classifier, mailer
from langchain.globals import set_verbose
import pickle
import json

# LangChain verbose ì„¤ì •
set_verbose(True)

st.set_page_config(page_title="ìƒë‹´ ê¸°ë¡ ë¶„ì„", layout="wide")

st.title("ğŸ“ ìƒë‹´ ê¸°ë¡ ë¶„ì„ ë°ëª¨")

# -----------------------
# 1. PDF ì—…ë¡œë“œ
# -----------------------
uploaded_file = st.file_uploader("ğŸ“„ ìƒë‹´ ê¸°ë¡ PDF ì—…ë¡œë“œ", type=["pdf"])

if uploaded_file:
    st.success("âœ… íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ!")

    # ì§„í–‰ ìƒíƒœ í‘œì‹œë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆ
    progress_container = st.empty()
    status_container = st.empty()

    try:
        # 2. ë¬¸ì„œ íŒŒì‹±
        with st.spinner("ë¬¸ì„œ ë¶„ì„ ì¤‘..."):
            status_container.info("ğŸ“„ PDF íŒŒì¼ íŒŒì‹± ì¤‘...")
            raw_text = document_parser.parse(uploaded_file)
            progress_container.progress(20, "ë¬¸ì„œ íŒŒì‹± ì™„ë£Œ")
        
        # 3. í…ìŠ¤íŠ¸ ì •ì œ ë° ìµëª…í™”
        status_container.info("ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ë° ìµëª…í™” ì¤‘...")
        # ì²­í¬ ìˆ˜ì— ë”°ë¼ ì§„í–‰ë¥  ê³„ì‚°
        chunks = text_cleaner.chunk_text(raw_text)
        total_chunks = len(chunks)
        
        def update_progress(chunk_num):
            base_progress = 20  # ì´ì „ ë‹¨ê³„ì˜ ì§„í–‰ë¥ 
            chunk_progress = (chunk_num / total_chunks) * 20  # ì²­í¬ ì²˜ë¦¬ ì§„í–‰ë¥ 
            total_progress = base_progress + chunk_progress
            progress_container.progress(int(total_progress), f"í…ìŠ¤íŠ¸ ì •ì œ ì¤‘... ({chunk_num}/{total_chunks})")
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜ë¥¼ ì „ë‹¬
        clean_text = text_cleaner.clean(raw_text, progress_callback=update_progress)
        progress_container.progress(40, "í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ")

        # 4. ìš”ì•½ 
        status_container.info("ğŸ“ ìƒë‹´ ë‚´ìš© ìš”ì•½ ì¤‘...")
        summary = summarizer.summarize(clean_text)
        progress_container.progress(60, "ìš”ì•½ ì™„ë£Œ")

        # 5. ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
        status_container.info("ğŸ” ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘...")
        all_results = []
        
        # ê° ì²­í¬ë³„ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
        for i, chunk in enumerate(chunks):
            try:
                chunk_results = search_faiss.search(chunk)
                if chunk_results:  # ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    all_results.extend(chunk_results)
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                chunk_progress = ((i + 1) / len(chunks)) * 20
                progress_container.progress(60 + int(chunk_progress), f"ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘... ({i+1}/{len(chunks)})")
            except Exception as e:
                st.error(f"ì²­í¬ {i+1} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                continue
        
        # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if not all_results:
            st.warning("ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            similar_cases = []
        else:
            # ê²°ê³¼ë¥¼ ì ìˆ˜ë³„ë¡œ ì •ë ¬í•˜ê³  ì¤‘ë³µ ì œê±°
            seen_docs = set()  # (file_path, text) íŠœí”Œì„ ì €ì¥
            similar_cases = []
            
            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ (ë†’ì€ ì ìˆ˜ê°€ ë” ìœ ì‚¬í•¨)
            sorted_results = sorted(all_results, key=lambda x: x.get('score', 0), reverse=True)
            
            for result in sorted_results:
                if not isinstance(result, dict):  # ê²°ê³¼ í˜•ì‹ ê²€ì¦
                    continue
                    
                # íŒŒì¼ ê²½ë¡œì™€ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì²´í¬
                doc_key = (result.get('source', ''), result.get('text', ''))
                if doc_key not in seen_docs and doc_key[0] and doc_key[1]:  # ë¹ˆ ê°’ ì œì™¸
                    seen_docs.add(doc_key)
                    similar_cases.append(result)
                    if len(similar_cases) >= 3:  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ ìœ ì§€
                        break
        
        progress_container.progress(80, "ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì™„ë£Œ")

        # 6. ë¶„ë¥˜
        status_container.info("ğŸ·ï¸ ìƒë‹´ ë¶„ë¥˜ ì¤‘...")
        classification = classifier.classify(summary, similar_cases)
        progress_container.progress(100, "ë¶„ë¥˜ ì™„ë£Œ")
        
        # ì§„í–‰ ìƒíƒœ ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™”
        progress_container.empty()
        status_container.empty()

        # -----------------------
        # streamlit UI
        # -----------------------

        # ì›ë³¸ í…ìŠ¤íŠ¸ (ì ‘ì„ ìˆ˜ ìˆëŠ” ì„¹ì…˜)
        with st.expander("ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸ ë³´ê¸°"):
            st.text_area("ì •ì œëœ í…ìŠ¤íŠ¸", clean_text, height=200)

        # êµ¬ì¡°í™”ëœ ìš”ì•½ ê²°ê³¼
        st.subheader("ğŸ§  ìƒë‹´ ìš”ì•½ ê²°ê³¼")
        st.write(summary)  # ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ í‘œì‹œ

        # ìœ ì‚¬ì‚¬ë¡€ ìš”ì•½ ì¹´ë“œ
        st.subheader("ğŸ” ìœ ì‚¬ ì‚¬ë¡€")
        for i, case in enumerate(similar_cases):
            with st.expander(f"ìœ ì‚¬ ì‚¬ë¡€ {i+1} (ìœ ì‚¬ë„: {case.get('score', 0):.2f})"):
                st.markdown(
                    f"""
                    <div style="border:1px solid #ddd; border-radius:12px; padding:16px; margin:8px;">
                        <h4>ìƒë‹´ ë‚´ìš©</h4>
                        <p>{case.get('text', 'ë‚´ìš© ì—†ìŒ')}</p>
                        
                        <h4>ìƒì„¸ ì •ë³´</h4>
                        <p><b>ID:</b> {case.get('id', 'ì •ë³´ ì—†ìŒ')}</p>
                        <p><b>ìƒë‹´ì¼:</b> {case.get('info', {}).get('ìƒë‹´ì¼', 'ì •ë³´ ì—†ìŒ')}</p>
                        <p><b>ìƒë‹´ì:</b> {case.get('info', {}).get('ìƒë‹´ì', 'ì •ë³´ ì—†ìŒ')}</p>
                        <p><b>ìƒë‹´ ìœ í˜•:</b> {case.get('info', {}).get('ìƒë‹´ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}</p>
                        <p><b>ìœ„í—˜ë„:</b> {case.get('info', {}).get('ìœ„í—˜ë„', 'ì •ë³´ ì—†ìŒ')}</p>
                        <p><b>í•™ëŒ€ ìœ í˜•:</b> {case.get('info', {}).get('í•™ëŒ€ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}</p>
                        
                        <h4>ë¬¸í•­ë³„ ì •ë³´</h4>
                        <pre style="background-color: #f5f5f5; padding: 10px; border-radius: 5px; overflow-x: auto;">
{json.dumps(case.get('ë¬¸í•­ë³„ì •ë³´', []), ensure_ascii=False, indent=2)}
                        </pre>
                    </div>
                    """, unsafe_allow_html=True
                )
                
                # ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                source = case.get('source')
                if source and os.path.exists(source):
                    with open(source, 'rb') as f:
                        file_data = f.read()
                        st.download_button(
                            label="ğŸ“¥ ì›ë³¸ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
                            data=file_data,
                            file_name=os.path.basename(source),
                            mime="application/json"
                        )

        # -----------------------
        # 7. ìœ„í—˜ ì•Œë¦¼
        if classification.get("emergency_level", 0) >= 3 or classification.get("abuse_type", "í•´ë‹¹ì—†ìŒ") != "í•´ë‹¹ì—†ìŒ":
            with st.spinner("ğŸš¨ ìœ„í—˜ ì•Œë¦¼ ë°œì†¡ ì¤‘..."):
                mailer.send_alert({
                    "type": classification.get("problem_type", ""),
                    "risk_level": classification.get("emergency_level", 0),
                    "abuse_type": classification.get("abuse_type", "í•´ë‹¹ì—†ìŒ"),
                    "timestamp": classification.get("timestamp", "")
                })
                st.warning("ğŸš¨ ìœ„í—˜ ìƒë‹´ ê°ì§€ë¨! ê´€ë¦¬ìì—ê²Œ ì•Œë¦¼ ë°œì†¡ë¨")

        # 8. ë¡œê·¸
        st.subheader("ğŸ•’ ì²˜ë¦¬ ë¡œê·¸")
        log_container = st.container()
        with log_container:
            st.markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼")
            st.markdown(f"""
            - **ìƒë‹´ ìœ í˜•**: {classification.get('type', 'ë¶„ë¥˜ë˜ì§€ ì•ŠìŒ')}
            - **ìœ„í—˜ë„**: {classification.get('risk_level', '0')}/5
            - **í•™ëŒ€ ìœ í˜•**: {classification.get('abuse_type', 'í•´ë‹¹ì—†ìŒ')}
            - **ì²˜ë¦¬ ì‹œê°„**: {classification.get('timestamp', '')}
            """)
            
            if classification.get("emergency_level", 0) >= 3 or classification.get("abuse_type", "í•´ë‹¹ì—†ìŒ") != "í•´ë‹¹ì—†ìŒ":
                st.markdown("### ğŸš¨ ìœ„í—˜ ì•Œë¦¼")
                st.markdown(f"""
                - **ì•Œë¦¼ ë°œì†¡**: âœ… ì™„ë£Œ
                - **ë°œì†¡ ì‹œê°„**: {classification.get('timestamp', '')}
                - **ì•Œë¦¼ ìœ í˜•**: {classification.get('problem_type', '')}
                - **ìœ„í—˜ ìˆ˜ì¤€**: {classification.get('risk_level', '0')}/5
                """)
            else:
                st.markdown("### âœ… ì •ìƒ ì²˜ë¦¬")
                st.markdown("ìœ„í—˜ ìˆ˜ì¤€ì´ ë‚®ì•„ ì•Œë¦¼ì´ ë°œì†¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    except FileNotFoundError as e:
        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.info("ğŸ’¡ í•´ê²° ë°©ë²•: faiss_index ë””ë ‰í† ë¦¬ì— í•„ìš”í•œ íŒŒì¼ë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")